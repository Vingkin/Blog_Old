---
title: 操作系统
author: Vingkin
date: 2022-4-24
---

> 0x01-0x05[再过60分钟你就能了解同步异步、阻塞非阻塞、IO多路复用、select、poll、epoll等概念啦 - 掘金 (juejin.cn)](https://juejin.cn/post/6971291445147729950)

## 0x01 用户空间和内核空间（用户态和内核态）

现在的操作系统都是采用虚拟存储器，对于32位操作系统而言，他的寻址空间（虚拟存储空间）为4G（$x^{32}$））。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作系统内核（kernel），保证内核的安全，操作系统将虚拟空间划分成两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节供内核使用，成为内核空间，而将较低的3G字节供各个进程使用，称为用户空间。

### 什么时候会从用户态切换到内核态？

1. 系统调用：用户进程主动切换到内核态的方式，用户态进程通过系统调用向操作系统申请资源完成工作，例如`fork()`就是一个创建新进程的系统调用，系统调用的机制核心使用了操作系统为用户特别开放的一个中断来实现，如Linux的 `int 80H` 中断，也可以称为软中断。
2. 异常：当CPU在执行用户态的进程时，发生了一些没有预知的异常，这时当前运行进程会切换到处理此异常的内核相关进程，也就是切换到了内核态，如缺页异常。
3. 中断：当CPU在执行用户态的进程时，外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂时执行下一个即将要执行的指令，转到与中断信号对应的处理程序去执行，也就是切换到了内核态。如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后边的操作等。

### 为什么用户态与内核态的转换开销大？

1. 保留用户态现场（上下文、寄存器、用户栈等）
2. 复制用户态参数，用户栈切换到内核栈，进入内核态
3. 额外的检查（内核代码对用户不信任）
4. 执行内核态代码
5. 复制内核态代码执行结果，回到用户态
6. 恢复用户态现场（上下文、寄存器、用户栈等）

## 0x02 文件描述符

文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。

文件描述符在形式上是一个非负整数。实际上，他是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一个概念往往只适用于UNIX、Linux这样的操作系统。

## 0x03 缓存 I/O

缓存I/O又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存（page cache）中，也就是说，**数据会被拷贝到操作系统内核的缓冲区中，然后才会从操作系统的内核缓冲区拷贝到应用程序的地址空间。**

**缓存I/O的缺点：**

数据在传输过程中需要在应用程序地址空间和内核之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。

## 0x04 I/O 模式

对于缓存I/O，对于一次I/O访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，他会经历两个阶段：

1. 等待数据准备
2. 将数据从内核拷贝到进程中

正是因为这两个阶段，linux系统产生了下面五种网络模式的方案

1. 阻塞I/O（blocking IO）
2. 非阻塞I/O（nonblocking IO）
3. I/O多路复用（IO multiplexing）
4. 信号驱动I/O（signal driven IO）
5. 异步I/O（asynchronous IO）

### 阻塞I/O

![阻塞IO](https://vingkin-1304361015.cos.ap-shanghai.myqcloud.com/interview/%E9%98%BB%E5%A1%9EIO.webp)

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程如上图所示。

当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，他就会将数据从kernel中拷贝到用户内存中，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

> blocking IO的特点就是IO执行的两个阶段都会被block

### 非阻塞I/O

![非阻塞IO](https://vingkin-1304361015.cos.ap-shanghai.myqcloud.com/interview/%E9%9D%9E%E9%98%BB%E5%A1%9EIO.webp)

在linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程如上图所示。

当用户发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，他就知道数据还没有准备好，于是他可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么他马上就将数据拷贝到了用户内存，然后返回。

> nonblocking IO的特点是用户进程需要不断地主动询问kernel数据好了没有

### I/O多路复用

![IO多路复用](https://vingkin-1304361015.cos.ap-shanghai.myqcloud.com/interview/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.webp)

IO multiplexing就是我们说的select，poll和epoll，有些地方也称这种IO方式为事件驱动IO（event driven IO）。select/epoll的好处就在于单个process就可以同时处理多个网络的IO。他的基本原理就是select，poll和epoll。不断地轮询所负责的所有socket，当某个socket有数据到达了就通知用户进程。

**当用户进程调用了select，那么整个进程会被block**，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

> IO多路复用的特点是通过一种机制使得一个进程能够同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select函数就可以返回

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call（select和recvfrom），而blocking IO只调用了一个system call（recvfrom）。但是，用select的优势在于它可以同时处理多个connection

所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。**select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。**

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。

### 异步I/O

![异步IO](https://vingkin-1304361015.cos.ap-shanghai.myqcloud.com/interview/%E5%BC%82%E6%AD%A5IO.webp)

Linux下的asynchronous IO其实用的很少。流程如上图所示。

用户进程发起read操作之后，立刻就可以开始去做其他事。而另一方面，从kernel的角度，当它收到一个asynchronous read之后，首先他会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉他read操作完成了。

**blocking和non-blocking的区别？**

调用blocking IO会一直block住对应的进程知道操作完成，而non-blocking IO在kernel准备数据的情况下会立刻返回。

### 不同I/O对比

![IO对比](https://vingkin-1304361015.cos.ap-shanghai.myqcloud.com/interview/IO%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.webp)

## 0x05 select、poll和epoll的区别

select，poll和epoll都是IO多路复用的机制。**IO多路复用就是通过一种机制使得一个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作**。但select，poll，epoll本质上都是同步IO，因为他们需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步IO则无需自己负责进行读写，异步IO的实现会负责把数据从内核空间拷贝到用户空间。

### select

select函数监视的文件描述符分3类，分别是writefds、readfds和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写或者except）或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过**遍历**fdset，来找到就绪的描述符。

select目前几乎在所有的平台上支持，其良好跨平台支持也是他的一个优点。select的一个缺点在于单个进程能够监视文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。

本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：

1. select最大的缺陷就是单个进程所打开的fd数量是有一定限制的，它由`FD_SETSIZE`设置，默认值是1024.一般来说这个数目和系统内存关系很大，具体数目可以`cat /proc/sys/fs/file-max`查看。32位机默认是1024个，64位机默认是2048个。
2. 对socket进行扫描时是线性扫描，采用轮询方式，效率较低。
3. 需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。

### poll

poll本质上和select没有区别。他将用户传入的fd数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前线程，知道设备就绪或者主动超时，被唤醒后他又要再次遍历fd。他没有最大连接数的限制，原因是他是基于链表来存储的，但是同样有缺点：

1. 大量的fd数组被整体复制于用户空间和内核空间，而不管这样的复制是否有意义
2. poll还有一个特点就是“水平触发”，如果报告了fd后没有被处理，那么下次poll时会再次报告该fd

> 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在同一时刻可能只有很少的处于就绪状态，因此随着监视的文件描述符数量的增长，其效率也会线性下降。

### epoll

epoll是select和poll的增强版本。相对于select和poll而言，epoll更加灵活，没有文件描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的时间存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

epoll支持水平出发和边缘触发，最大的特点就在于边缘触发，他只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。

**epoll的优点**

1. 没有最大并发连接的限制，能打开的fd的上限远大于1024（1G的内存大约能监听10万个端口）
2. 效率提升，不是轮询的方式，不会随着fd数目的增加使得效率降低。只有活跃可用的fd才会调用callback函数。即epoll最大的优点就在于他只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，epoll的效率就会远远高于select和poll
3. 内存拷贝，利用mmap()文件映射内存加速和内核空间的消息传递。即epoll使用mmap减少复制开销

**水平触发和边缘触发(Level trigger / edge trigger)**

epoll对文件描述符的操作有两种模式：LT(Level Trigger)和ET(Edge Trigger)。LT模式是默认模式，LT和ET的区别如下：

1. LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件下次调用epoll_wait时，会再次相应应用程序并通知此事件。
2. ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次相应应用程序并通知此事件。

> ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，防止由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 总结

1. 在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描。
2. epoll实现通过epoll_ctl()来注册一个文件扫描符，一旦基于某个文件扫描符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。
3. 如果没有大量的闲置连接和死亡连接，那么epoll的效率并不会比select/poll高很多，但是如果很多的话，就会发现epoll的效率大大高于select/poll。



|            | 支持一个进程所能打开的最大连接数                             | fd剧增后带来的IO效率问题                                     | 消息传递方式                                       |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------- |
| **select** | 单个进程所能打开的最大连接数由FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上大小就是32\*32，64位机器上就是32\*64），当然我们可以对其进行修改，然后重新编译内核但是性能可能受到影响，这需要进一步测试 | 因为每次调用时都会对连接进行线性遍历，所以随着fd的增加会造成遍历速度慢的“线性下降性能问题” | 内核需要将消息传递到用户空间，都需要内核的拷贝动作 |
| **poll**   | poll本质上和select没有区别，但是他没有最大连接数的限制，原因是他是基于链表来存储的 | 同上                                                         | 同上                                               |
| **epoll**  | 虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接 | 因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以在活跃的socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能有性能问题 | epoll通过内核空间和用户空间共享一块内存来实现      |

在选择select，poll和epoll时要根据具体的使用场合以及这三种方式的自身特点：

1. 表面上epoll的性能更好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调
2. select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善

